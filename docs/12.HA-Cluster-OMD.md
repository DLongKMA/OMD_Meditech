# Cấu hình Cluster cho check_mk - OMD

### Menu

- [1. Chuẩn bị](#1)
	- [Yêu cầu hệ thống](#11)
	- [Mô hình triển khai](#12)
	- [Bảng phân hoạch IP](#13)
- [2. Các bước cài đặt](#2)
	- [2.1 Đồng nhất thời gian giữa 2 server](#21)
	- [2.2 Cấu hình LVM cho ổ cứng mới gắn](#22)
	- [2.3 Cấu hình hostname](#23)
	- [2.4](#24)
	- [2.5](#25)
- [3. Tham khảo](#3)

<a name="1" />
	
## 1. Chuẩn bị

<a name="11" />

- **Yêu cầu hệ thống:**
	- Hệ điều hành CentOS 7
	- Mỗi server gắn thêm một ổ cứng 5GB
	- Mỗi server có 2 card mạng
	- Tắt SELinux trên cả 2 server
	- Đồng nhất thời gian trên 2 server
	- Mở các port sau:
		- Apache: 80
		- DRBD: 5778

<a name="12" />

- **Mô hình triển khai**

<img src="/images/HA-OMD-check_mk.png" width=70% />

<a name="13" />

- **Bảng phân hoạch IP cho các máy chủ**

<img src="/images/HA-omd-ip.png" />

Link Online: https://goo.gl/N8FFmz

- Chú thích:
	- Dải 192.168.100.0/24 phục vụ cho việc ra ngoài Internet
	- Dải 172.16.1.0/24 phục vụ cho việc đồng bộ dữ liệu giữa 2 server
	
<a name="2" />

## 2. Các bước cài đặt

<a name="21" />

### 2.1 Đồng nhất thời gian giữa 2 server

- Để cấu hình được cluster, chúng ta sẽ phải đồng nhất thời gian cho 2 server. Trên cả 2 server, chọn múi giờ `Asia/Ho_Chi_Minh` và cài gói `ntp` để đồng bộ thời gian.

```
timedatectl set-timezone Asia/Ho_Chi_Minh
yum install ntp -y
ntpdate pool.ntp.org
```

<a name="22" />

### 2.2 Cấu hình LVM trên 2 server

- Như liệt kê ở phần chuẩn bị, chúng ta gắn thêm một ổ cứng 5GB và cấu hình trên mỗi server.
- Trên cả 2 node, chúng ta tạo một LVM để lưu trữ các dữ liệu có tên là `/dev/vgdrbd/vol1`. Ở đây, /dev/sdb được sử dụng để tạo LVM.

```
[root@omd1 ~]# pvcreate /dev/sdb
  Physical volume "/dev/sdb" successfully created
[root@omd1 ~]# vgcreate vgdrbd /dev/sdb
  Volume group "vgdrbd" successfully created
[root@omd1 ~]# lvcreate -n vol1 -l100%FREE vgdrbd
  Logical volume "vol1" created.
```

<a name="23" />

### 2.3 Cấu hình hostname

Trước khi cài đặt, chúng ta phải cấu hình hostname cho mỗi node và ghi chúng vào``hosts`.

- **Bước 1:** Cấu hình hostname

Trên server 1

```
[root@node1 ~] hostnamectl set-hostname omd1
```

Trên server 2

```
[root@node2 ~] hostnamectl set-hostname omd2
```

- **Bước 2:** Ghi thêm vào hosts của mỗi server

```
vi /etc/hosts
```

```
[...]
192.168.100.135 omd1
192.168.100.136 omd2
172.16.1.135 omddata1
172.16.1.136 omddata2
```

<a name="24" />

### 2.4 Cài đặt và cấu hình DRBD trên 2 server

- **Bước 1:** Cài đặt DRBD

	- Để cài đặt DRDB mới nhất, chúng ta thêm repo của DRDB (các thao tác này làm trên cả 2 node):

	```
	rpm -ivh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
	```
	- Thêm Public key cho DRDB

	```
	rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-elrepo.org
	```
	- Tiếp theo, chúng ta cài đặt DRDB lần lượt trên cả 2 node:

	```
	yum -y install drbd84-utils kmod-drbd84
	```

	- Nếu quá trình cài đặt trên không thành công hoặc xảy ra lỗi hãy chèn lại key và tiếp tục cài đặt:

	```
	rpm --import /etc/pki/rpm-gpg/*
	```

	- Kích hoạt DRDB trên cả 2 node

	```
	modprobe drbd
	```

	- Kiểm tra DRDB đã hoạt động hay chưa:

	```
	lsmod | grep drbd

	drbd                  405309  0
	libcrc32c              12644  2 xfs,drbd
	```

- **Bước 2:** Cấu hình DRBD

	Chúng tạo một file có tên `testdata1.res` với nội dung như sau:

	```
	vi /etc/drbd.d/testdata1.res
	```

	```
	resource testdata1 {
	protocol C;          
	on omddata1 {
					device /dev/drbd0;
					disk /dev/vgdrbd/vol1;
					address 172.16.1.135:7788;
					meta-disk internal;
			}
	on omddata2 {
					device /dev/drbd0;
					disk /dev/vgdrbd/vol1;
					address 172.16.1.136:7788;
					meta-disk internal;
			}
	} 
	```
	
	Sao chép file cấu hình sang server 2:

	```
	[root@omd1 ~] scp /etc/drbd.d/testdata1.res node2:/etc/drbd.d/
	```

- **Bước 3:** Tạo và khởi động thiết bị DRBD	
		
	Khởi động trên mỗi node

	Trên server 1:

	```
	[root@omd1 ~] drbdadm create-md testdata1
	```

	Trên server 2:

	```
	[root@omd2 ~] drbdadm create-md testdata1
	```

	Khi thấy kết quả hiển thị như sau báo hiệu đã cấu hình thành công:

	```
	  --==  Thank you for participating in the global usage survey  ==--
	The server's response is:
	you are the 10680th user to install this version
	initializing activity log
	NOT initializing bitmap
	Writing meta data...
	New drbd meta data block successfully created.
	success
	```
	
	Ở trên 2 server, chúng ta bật và cho DRBD khởi động cùng hệ thống

	```
	systemctl start drbd
	systemctl enable drbd
	```

- **Bước 4:** Kích hoạt thiết bị DRBD

2.5 Kích hoạt trên node chính

	Chúng tôi chọn node chính là `omd1`, chúng ta cũng có thể chọn `omd2` làm node chính bằng cách chạy lệnh này lên `omd2`.

	```
	[root@omd1 ~] drbdadm primary testdata1 --force
	```
	Kiểm tra trạng thái:

	```
	[root@omd1 ~]# cat /proc/drbd
	version: 8.4.7-1 (api:1/proto:86-101)
	GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by phil@Build64R7, 2016-01-12 14:29:40
	 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
		ns:1048508 nr:0 dw:0 dr:1049236 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
	```

	Hoặc dùng lệnh:

	```
	[root@omd1 ~]# drbd-overview
	 0:testdata1/0  Connected Primary/Secondary UpToDate/UpToDate
	```
	
- **Bước 5:** Cấu hình tường lửa trên cả 2 node
	
	```
	firewall-cmd --permanent --add-port=7788/tcp
	firewall-cmd --reload
	```
	
<a name="25" />
	
### 2.5 Cài đặt và cấu hình Pacemaker và Corosync trên 2 server

#### Cài đặt Pacemaker và Corosync

- **Bước 1:** Cài đặt các gói cluster trên cả 2 node bằng `yum`:

```
yum install -y pacemaker pcs fence-agents-all psmisc policycoreutils-python
```

- **Bước 2:** Chúng ta cấu hình tường lửa cho phép dịch vụ HA:

```
firewall-cmd --permanent --add-service=high-availability
firewall-cmd --reload
```

#### Cấu hình Cluster với Pacemaker và Corosync

- **Bước 1:** Tạo mật khẩu cho user `hacluster`

Trên cả 2 node, chúng ta đặt password cho user `hacluster` để xác thực với nhau, 2 mật khẩu phải trùng khớp.

```
echo "redhat" | passwd --stdin hacluster
```

- **Bước 2:** Khởi động dịch vụ trên cả 2 node:

```
systemctl start pcsd
systemctl enable pcsd
```

- **Bước 3:** Xác thực giữa 2 node 

Trên `omd1`, chúng ta xác thực 2 node với nhau bằng lệnh:

```
[root@omd1 ~]# pcs cluster auth omd1 omd2 -u hacluster -p redhat
omd1: Authorized
omd2: Authorized
```

- **Bước 4:** Tạo mới 1 cluster

Sau khi xác thực, chúng ta tạo 1 cluster trên omd1 có tên là `mycluster` để chúng có thể tạo và đồng bộ các file cấu hình với nhau.

```
[root@omd1 ~]# pcs cluster setup --name mycluster omd1 omd2
Shutting down pacemaker/corosync services...
Redirecting to /bin/systemctl stop  pacemaker.service
Redirecting to /bin/systemctl stop  corosync.service
Killing any remaining services...
Removing all cluster configuration files...
omd1: Succeeded
omd2: Succeeded
Synchronizing pcsd certificates on nodes omd1, omd2...
omd1: Success
omd2: Success
Restaring pcsd on the nodes in order to reload the certificates...
omd1: Success
omd2: Success
```

- **Bước 5:** Khởi động và kích hoạt cluster mới tạo trên omd1 bằng lệnh:

```
[root@omd1 ~]# pcs cluster start --all
[root@omd1 ~]# pcs cluster enable --all
```

Xem lại trạng thái của cluster trên các node:

```
pcs status
```

- **Bước 6:** Tắt Quorum và STONITH

```
pcs property set stonith-enabled=false
pcs property set no-quorum-policy=ignore
pcs property set default-resource-stickiness="INFINITY"
```
	
<a name="3" />